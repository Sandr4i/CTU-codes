{"cells":[{"cell_type":"markdown","metadata":{"id":"HV_h0NBNrh6G"},"source":["# Lab 3: Partially Observable Markov Decision Process (POMDP)\n","\n","In the previous lab, we studied the MDP model:\n","\n","- $\\mathcal S$ - state space\n","- $\\mathcal A$ - possible actions\n","- $T$ - transition function\n","- $R$ - reward function\n","\n","POMDP extends MDP with:\n","\n","- set of possible observations,\n","- observation function or distribution O(o|s',a),\n","- and initial belief distribution over the states.\n","\n","The goal is to maximize reward in a partially observable environment.\n","\n","- Agent cannot fully observe current state but instead has belief in form of probability distribution over the entire state space.\n","- For each action and new state agent receives an observation.\n","- Agent updates its belief based on the previous belief, action and observation of the new state.\n","\n","\n","Example:\n","\n","- Maze: Agent only see few tiles around him. (Maze is foggy)\n","- Can you think of other examples?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qcI3VTrjrh6I"},"source":["## Task 1\n","\n","Let's use the maze again but differently:\n","\n","- Direction of agent is now part of the state. Agent in the maze is represented by characters `<` `>` `^` `v`  based on its direction, instead of `@`\n","- Agent actions (turn left `l`, turn right `r`) can change its direction.\n","- Only movement forward `m` is allowed.\n","\n","Implement function `apply` in this new setup."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"sWaaPOPurh6I"},"outputs":[],"source":["from copy import deepcopy\n","from typing import List, Tuple, Dict, Set\n","\n","\n","Action = str  # Type alias\n","\n","\n","class State:\n","\n","    def __init__(self, maze_rows: List[str]):\n","        self._maze_rows = [list(row) for row in maze_rows]\n","\n","        # Make sure the maze has a rectangular shape, and has a proper boundary.\n","        assert all(len(row) == self.num_cols() for row in self._maze_rows)\n","        assert set(self._maze_rows[0]) == {\"#\"} and set(self._maze_rows[-1]) == {\"#\"}\n","        assert all(row[0] == \"#\" and row[-1] == \"#\" for row in self._maze_rows)\n","\n","    def actions(self) -> List[Action]:\n","        \"\"\"\n","        :return: list of actions available at the current state.\n","        \"\"\"\n","        return [\n","            \"m\",  # Move forward.\n","            \"l\",  # Turn left.\n","            \"r\",  # Turn right.\n","        ]\n","\n","    def apply(self, action: Action) -> None:\n","        i, j, direction = self.current_position()\n","        m = self._maze_rows\n","        \n","        if action == \"m\":\n","            movement_map = {\n","                '^': [-1, 0],\n","                'v': [1, 0],\n","                '<': [0, -1],\n","                '>': [0, 1]}\n","            di, dj = movement_map.get(direction)\n","            i_new = i + di\n","            j_new = j + dj\n","            if m[i_new][j_new] != \"#\":\n","                m[i][j], m[i_new][j_new] = \" \", m[i][j]\n","        else:\n","            direction_map = {\n","                \"l\": {\"^\": \"<\", \"<\":\"v\", \"v\": \">\", \">\": \"^\"},\n","                \"r\": {\"^\": \">\", \">\":\"v\", \"v\": \"<\", \"<\": \"^\"},}\n","            new_direction = direction_map[action][direction]\n","            m[i][j] = new_direction\n","\n","    def copy(self) -> \"State\":\n","        return State(deepcopy(self._maze_rows))\n","\n","    # -- Maze specific methods ---------------------------------------------------\n","\n","    def num_rows(self) -> int:\n","        return len(self._maze_rows)\n","\n","    def num_cols(self) -> int:\n","        return len(self._maze_rows[0])\n","\n","    def current_position(self) -> Tuple[int, int, str]:\n","        # Return agent coordinates and orientation.\n","        for i in range(self.num_rows()):\n","            for j in range(self.num_cols()):\n","                if self._maze_rows[i][j] in [\"^\", \"<\", \"v\", \">\"]:\n","                    return i, j, self._maze_rows[i][j]\n","        raise RuntimeError(\n","            \"Invalid maze: current position not found in: \" + \"\\n\".join(self._maze_rows)\n","        )\n","\n","    def num_golds(self) -> int:\n","        golds = 0\n","        for i in range(self.num_rows()):\n","            for j in range(self.num_cols()):\n","                if self._maze_rows[i][j] == \"G\":\n","                    golds += 1\n","        return golds\n","\n","    def has_any_gold(self) -> bool:\n","        return self.num_golds() > 0\n","\n","    # -- Helper methods ----------------------------------------------------------\n","\n","    def __str__(self) -> str:\n","        return \"\\n\".join([\"\".join(row) for row in self._maze_rows])\n","\n","    def __eq__(self, other) -> bool:\n","        return self._maze_rows == other._maze_rows\n","\n","    def __hash__(self) -> int:\n","        return hash(str(self))\n","\n","    def __repr__(self) -> str:\n","        return str(self)\n"]},{"cell_type":"markdown","metadata":{"id":"GIlKbt4vBF72"},"source":["We provide a number of test cases your implementation must pass:"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"fL7eM2Bcrh6J"},"outputs":[],"source":["def test_apply_turn() -> None:\n","    test_maze = [\n","        \"#####\",\n","        \"# ^ #\",\n","        \"#####\"]\n","\n","    maze = State(test_maze)\n","    maze.apply(\"l\")\n","    result_maze = [\n","        \"#####\",\n","        \"# < #\",\n","        \"#####\" ]\n","    assert maze == State(result_maze)\n","\n","    maze = State(test_maze)\n","    maze.apply(\"r\")\n","    result_maze = [\n","        \"#####\",\n","        \"# > #\",\n","        \"#####\" ]\n","    assert maze == State(result_maze)\n","\n","    maze = State(test_maze)\n","    maze.apply(\"l\")\n","    maze.apply(\"l\")\n","    result_maze = [\n","        \"#####\",\n","        \"# v #\",\n","        \"#####\" ]\n","    assert maze == State(result_maze)\n","\n","    maze = State(test_maze)\n","    maze.apply(\"l\")\n","    maze.apply(\"r\")\n","    result_maze = [\n","        \"#####\",\n","        \"# ^ #\",\n","        \"#####\" ]\n","    assert maze == State(result_maze)\n","\n","def test_apply_move() -> None:\n","    test_maze = [\n","        \"#####\",\n","        \"# ^ #\",\n","        \"#####\"]\n","\n","    maze = State(test_maze)\n","    maze.apply(\"m\")\n","    result_maze = [\n","        \"#####\",\n","        \"# ^ #\",\n","        \"#####\" ]\n","    assert maze == State(result_maze)\n","\n","    test_maze = [\n","        \"#####\",\n","        \"# > #\",\n","        \"#####\"]\n","    maze = State(test_maze)\n","    maze.apply(\"m\")\n","    result_maze = [\n","        \"#####\",\n","        \"#  >#\",\n","        \"#####\" ]\n","    assert maze == State(result_maze)\n","\n","\n","def test_apply_move_gold() -> None:\n","    test_maze = [\n","        \"#####\",\n","        \"# >G#\",\n","        \"#####\"]\n","    maze = State(test_maze)\n","    assert maze.num_golds() == 1\n","    maze.apply(\"m\")\n","    result_maze = [\n","        \"#####\",\n","        \"#  >#\",\n","        \"#####\" ]\n","    assert maze == State(result_maze)\n","    assert maze.num_golds() == 0\n","\n","# Pass tests\n","test_apply_turn()\n","test_apply_move()\n","test_apply_move_gold()"]},{"cell_type":"markdown","metadata":{"id":"ZsxjMJG-rh6M"},"source":["# Models\n","\n","\n","In Lab2, we had a transition model in the form of a `SxSxA` matrix and a reward model in the form of a `SxA` matrix. Keeping all transitions and rewards in the memory can cause memory issues for large problems. In the case of POMDPs, we need to have yet another matrix for all the observations (`SxA` matrix in general case).\n","\n","A different approach we will now use is to procedurally generate transitions, rewards and observation only when needed, based on known rules. In this way, we don't need to keep the whole matrix in memory all the time."]},{"cell_type":"markdown","metadata":{"id":"POdo5vocrh6N"},"source":["### Solver library\n","- To solve instances of POMDP, we will use `pomdp_py` library.\n","\n","- The library uses `pomdp_py.State`, `pomdp_py.Observation`, `pomdp_py.Action` classes to represent the corresponding concepts. These create an interface we should work with in order to use the library -- therefore we will wrap our implementation inside of these classes.\n","\n","- Additionally, we need to define `TransitionModel`, `RewardModel` and `ObservationModel` classes to model corresponding distributions.\n","\n","- We need function `get_all_states` to create uniform prior belief over all states."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"n6GsCapXrh6M"},"outputs":[{"name":"stdout","output_type":"stream","text":["^C\n","Traceback (most recent call last):\n","  File \"/bin/pip\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/lib/python3/dist-packages/pip/_internal/cli/main.py\", line 68, in main\n","    command = create_command(cmd_name, isolated=(\"--isolated\" in cmd_args))\n","  File \"/usr/lib/python3/dist-packages/pip/_internal/commands/__init__.py\", line 109, in create_command\n","    module = importlib.import_module(module_path)\n","  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n","    return _bootstrap._gcd_import(name[level:], package, level)\n","  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n","  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n","  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n","  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n","  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n","  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n","  File \"/usr/lib/python3/dist-packages/pip/_internal/commands/install.py\", line 14, in <module>\n","    from pip._internal.cli.req_command import (\n","  File \"/usr/lib/python3/dist-packages/pip/_internal/cli/req_command.py\", line 20, in <module>\n","    from pip._internal.index.collector import LinkCollector\n","  File \"/usr/lib/python3/dist-packages/pip/_internal/index/collector.py\", line 39, in <module>\n","    from pip._internal.network.session import PipSession\n","  File \"/usr/lib/python3/dist-packages/pip/_internal/network/session.py\", line 31, in <module>\n","    from pip._internal.network.auth import MultiDomainBasicAuth\n","  File \"/usr/lib/python3/dist-packages/pip/_internal/network/auth.py\", line 29, in <module>\n","    import keyring\n","  File \"/usr/lib/python3/dist-packages/keyring/__init__.py\", line 1, in <module>\n","    from .core import (\n","  File \"/usr/lib/python3/dist-packages/keyring/core.py\", line 11, in <module>\n","    from . import backend, credentials\n","  File \"/usr/lib/python3/dist-packages/keyring/backend.py\", line 12, in <module>\n","    import importlib_metadata as metadata\n","  File \"/usr/lib/python3/dist-packages/importlib_metadata/__init__.py\", line 17, in <module>\n","    from . import _adapters, _meta\n","  File \"/usr/lib/python3/dist-packages/importlib_metadata/_adapters.py\", line 5, in <module>\n","    from ._text import FoldedCase\n","  File \"/usr/lib/python3/dist-packages/importlib_metadata/_text.py\", line 3, in <module>\n","    from ._functools import method_cache\n","  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n","  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n","  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n","  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n","  File \"<frozen importlib._bootstrap_external>\", line 975, in get_code\n","  File \"<frozen importlib._bootstrap_external>\", line 1074, in get_data\n","KeyboardInterrupt\n","Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: pomdp-py in /home/sandrai/.local/lib/python3.10/site-packages (1.3.4)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/lib/python3/dist-packages (from pomdp-py) (1.8.0)\n","Requirement already satisfied: tqdm>=4.55.0 in /home/sandrai/.local/lib/python3.10/site-packages (from pomdp-py) (4.66.2)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/lib/python3/dist-packages (from pomdp-py) (1.21.5)\n"]}],"source":["# Install library with pomdp solver (takes a while) ...\n","!pip install cython\n","!pip install pomdp-py"]},{"cell_type":"markdown","metadata":{"id":"P_wb604uBF74"},"source":["We need to implement interface for the pomdp library:"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"CnS6nfTbBF74"},"outputs":[],"source":["import pomdp_py"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"-t6zbWoGBF74"},"outputs":[],"source":["# This is boilerplate code that wraps our State and Action\n","# for the use with the external library.\n","\n","class PomdpAction(pomdp_py.Action):\n","    def __init__(self, value: Action) -> None:\n","      self.value = value\n","\n","    def __hash__(self) -> int:\n","      return hash(self.value)\n","\n","    def __eq__(self, other) -> bool:\n","      return self.value == other.value\n","\n","    def __str__(self) -> str:\n","      return self.value\n","\n","    def __repr__(self) -> str:\n","      return str(self)\n","\n","\n","class PomdpState(pomdp_py.State):\n","    def __init__(self, maze_rows: List[str]) -> None:\n","        super().__init__()\n","        self.state = State(maze_rows)\n","        self._maze_rows = self.state._maze_rows\n","\n","    def apply(self, action: PomdpAction) -> None:\n","      self.state.apply(action.value)\n","\n","    def actions(self) -> List[PomdpAction]:\n","      return [PomdpAction(\"m\"), PomdpAction(\"l\"), PomdpAction(\"r\")]\n","\n","    def current_position(self) -> Tuple[int, int, str]:\n","      return self.state.current_position()\n","\n","    def num_golds(self) -> int:\n","      return self.state.num_golds()\n","\n","    def has_any_gold(self) -> bool:\n","      return self.state.has_any_gold()\n","\n","    def copy(self) -> \"PomdpState\":\n","      return PomdpState(deepcopy(self.state._maze_rows))\n","\n","    def __hash__(self) -> int:\n","      return hash(self.state)\n","\n","    def __eq__(self, other: \"PomdpState\") -> bool:\n","      return self.state == other.state\n","\n","    def __str__(self) -> str:\n","      return str(self.state)\n","\n","    def __repr__(self) -> str:\n","      return repr(self.state)\n","\n","\n","class PomdpObservation(pomdp_py.Observation):\n","    def __init__(self, value) -> None:\n","      self.value = value\n","\n","    def __hash__(self) -> int:\n","      return hash(self.value)\n","\n","    def __eq__(self, other: \"PomdpState\") :\n","      return self.value == other.value\n","\n","    def __str__(self) -> str:\n","      return self.value\n","\n","    def __repr__(self) -> str:\n","      return str(self)\n","\n","\n","class PolicyModel(pomdp_py.RandomRollout):\n","    def sample(self, state, **kwargs) -> Action:\n","        return self.get_all_actions().random()\n","\n","    def get_all_actions(self, **kwargs) -> List[Action]:\n","        return [PomdpAction(\"m\"), PomdpAction(\"l\"), PomdpAction(\"r\")]"]},{"cell_type":"markdown","metadata":{"id":"Ot-qeZ3DBF74"},"source":["**From now on, we will use these wrapped classes**: `PomdpAction`, `PomdpState`, `PomdpObservation`, `PolicyModel`"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"2WTA_4HMBF74"},"outputs":[],"source":["# Slightly modified function get_all_transitions() from Lab 2.\n","\n","def get_all_states(init_state: PomdpState) -> List[PomdpState]:\n","    states = [init_state]\n","    opened = [0]\n","\n","    while len(opened) > 0:\n","        from_idx = opened.pop()\n","        state = states[from_idx]\n","\n","        for action in state.actions():\n","            next_state = state.copy()\n","            next_state.apply(action)\n","            if next_state in states:\n","                # State was visited.\n","                to_idx = states.index(next_state)\n","            else:\n","                # Not visited yet, add to open.\n","                states.append(next_state)\n","                to_idx = len(states) - 1\n","                opened.append(to_idx)\n","    return states"]},{"cell_type":"markdown","metadata":{"id":"vUQCuttUrh6O"},"source":["\n","# Task 2\n","\n","Implement probability(**next_state**, **state**, **action**) method of transition model.\n","\n","In our deterministic case probability is always 1 (if **action** from **state** leads to **next_state**) or 0 (if not)."]},{"cell_type":"code","execution_count":34,"metadata":{"id":"2eGDJ5m6rh6P"},"outputs":[],"source":["class TransitionModel(pomdp_py.TransitionModel):\n","    \"\"\"\n","    Models the distribution T(s, a, s') = Pr(s'|s,a).\n","    In our deterministic case Pr(s'|s,a) is always 1\n","    (if s,a goes to s') or 0 (if not).\n","    \"\"\"\n","\n","    def probability(\n","        self,\n","        next_state: PomdpState,\n","        state: PomdpState,\n","        action: PomdpAction\n","    ) -> float:\n","        state_copy = state.copy()\n","        state_copy.apply(action)\n","        if (state_copy == next_state):\n","            return 1.0\n","        else:\n","            return 0.0\n","            \n","\n","    def sample(\n","          self,\n","          state: PomdpState,\n","          action: PomdpAction\n","    ) -> PomdpState:\n","        \"\"\"\n","        Given state and action, return next state.\n","        \"\"\"\n","        next_state = state.copy()\n","        next_state.apply(action)\n","        return next_state\n","\n","    def get_all_states(self) -> List[PomdpState]:\n","        return self.states"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"MY0QJk--rh6P"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.0\n","1.0\n","######\n","# < G#\n","######\n"]}],"source":["# Example. Try to predict the output values before running!\n","\n","tm = TransitionModel()\n","\n","state1 = PomdpState([\n","    \"######\",\n","    \"# ^ G#\",\n","    \"######\"])\n","\n","state2 = PomdpState([\n","    \"######\",\n","    \"# > G#\",\n","    \"######\"])\n","\n","print(tm.probability(state2, state1, PomdpAction(\"l\")))\n","print(tm.probability(state2, state1, PomdpAction(\"r\")))\n","print(tm.sample(state1, PomdpAction(\"l\")))\n"]},{"cell_type":"markdown","metadata":{"id":"LpXTapWjrh6P"},"source":["# Task 3\n","\n","Implement sample(***state***, ***action***) method of reward model.\n","\n","If ***action*** leads to ***state*** with gold, reward is 1. In all other cases, reward is -0.1\n","Hint: use gold attribute from Maze."]},{"cell_type":"code","execution_count":38,"metadata":{"id":"U25IqIkurh6P"},"outputs":[],"source":["class RewardModel(pomdp_py.RewardModel):\n","    \"\"\"\n","    Models rewards.\n","    In our case a reaward is 1 (if a leads to gold state)\n","    or -0.1 (if not).\n","    \"\"\"\n","\n","    def sample(\n","        self,\n","        state: PomdpState,\n","        action: PomdpAction,\n","        next_state=None\n","    ) -> float:\n","        \"\"\"\n","        Given state and action, return reward.\n","        Next state is ignored in our Maze env.\n","        \"\"\"\n","        \n","        new_state = state.copy()\n","        new_state.apply(action)\n","        if(new_state.num_golds() < state.num_golds()):\n","            return 1.0\n","        else:\n","            return -0.1\n","        \n"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"8eiAWq6Trh6Q"},"outputs":[{"name":"stdout","output_type":"stream","text":["#####\n","# ^G#\n","#####\n","Action r Reward -0.1\n","#####\n","# >G#\n","#####\n","\n","\n","#####\n","# >G#\n","#####\n","Action m Reward 1.0\n","#####\n","#  >#\n","#####\n"]}],"source":["# Example:\n","\n","rm = RewardModel()\n","maze = PomdpState([\n","    \"#####\",\n","    \"# ^G#\",\n","    \"#####\"])\n","\n","action = PomdpAction(\"r\")\n","print(maze)\n","print(f'Action {action} Reward {rm.sample(maze, action, None)}')\n","maze.apply(action)\n","print(maze)\n","print('\\n')\n","\n","action = PomdpAction(\"m\")\n","print(maze)\n","print(f'Action {action} Reward {rm.sample(maze, action, None)}')\n","maze.apply(action)\n","print(maze)\n"]},{"cell_type":"markdown","metadata":{"id":"gbl16eu6rh6R"},"source":["# Task 4\n","\n","\n","Implement function observe(**state**).\n","\n","- Function should output string of 4 character based on building blocks of maze, i.e.  one of `'#', ' ', 'G'`\n","- Example output: `\" # #\"`\n","- Function output depends on the state and uses the direction of the agent. (Select characters clockwise, starting with the character in front of the agent.)\n"]},{"cell_type":"markdown","metadata":{"id":"pfGu3ktTBF75"},"source":["Examples. What are the observations of the agent?\n","\n","```\n","##1##\n","#4^2#\n","##3##\n","```\n","\n","\n","\n","```\n","#####\n","# ^G#\n","#####\n","```\n","\n","\n","```\n","#####\n","# >G#\n","#####\n","```\n","\n"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"UFDPPgK_rh6R"},"outputs":[],"source":["class ObservationModel(pomdp_py.ObservationModel):\n","    '''\n","    Models distribution O(s',a,o) = Pr(o|s',a)\n","    In our case, observation does not depend on action -  Pr(o|s')\n","    (All actions leads to some observation).\n","\n","    Our Observation model is deterministic: Pr(o|s') is always 1\n","    (if observation of s' matches o) or 0 (if not).\n","    '''\n","\n","    def probability(\n","        self,\n","        observation: PomdpObservation,\n","        next_state: PomdpState,\n","        action: PomdpAction=None\n","    ) -> float:\n","        '''\n","        Returns the probability Pr(o|s',a).\n","        '''\n","        next_state_observation = self.sample(next_state)\n","        if next_state_observation == observation:\n","            return 1.0\n","        else:\n","            return 0.0\n","\n","    def observe(self, state: PomdpState) -> str:\n","        '''\n","        Agent looks around clockwise and returns string of 4 characters.\n","        '''\n","        copy = state.copy()\n","        i, j, d = copy.current_position()\n","        observation_map = {\n","            '^': [[-1, 0], [0, 1], [1, 0], [0, -1]],\n","            '>': [[0, 1], [1, 0], [0, -1], [-1, 0]],\n","            'v': [[1, 0], [0, -1], [-1, 0], [0, 1]],\n","            '<': [[0, -1], [-1, 0], [0, 1], [1, 0]], }\n","        observation = []\n","        for o in observation_map[d]:\n","            i_new = i + o[0]\n","            j_new = j + o[1]\n","            character = state._maze_rows[i_new][j_new]\n","            observation.append(character)\n","        return ''.join(observation)\n","\n","\n","    def sample(\n","        self,\n","        next_state: PomdpState,\n","        action: PomdpAction=None\n","        ) -> PomdpObservation:\n","        return PomdpObservation(self.observe(next_state))\n"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"l45k9SZjrh6R"},"outputs":[],"source":["def test_observation():\n","    om = ObservationModel()\n","    maze = PomdpState([\n","        \"#####\",\n","        \"# ^G#\",\n","        \"#####\"])\n","    assert str(om.sample(maze)) == '#G# '\n","\n","    maze = PomdpState([\n","        \"#####\",\n","        \"# >G#\",\n","        \"#####\"])\n","    assert str(om.sample(maze)) == 'G# #'\n","\n","    maze = PomdpState([\n","        \"#####\",\n","        \"# vG#\",\n","        \"#####\"])\n","    assert str(om.sample(maze)) == '# #G'\n","\n","    maze = PomdpState([\n","        \"#####\",\n","        \"# <G#\",\n","        \"#####\"])\n","    assert str(om.sample(maze)) == ' #G#'\n","\n","    assert om.probability(om.sample(maze), maze) == 1.\n","\n","test_observation()"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"VdhLPT1srh6R"},"outputs":[{"name":"stdout","output_type":"stream","text":["# ##\n"]}],"source":["# Example\n","om = ObservationModel()\n","maze = State([\n","    \"####\",\n","    \"#^ #\",\n","    \"####\"])\n","print(om.sample(maze))"]},{"cell_type":"markdown","metadata":{"id":"NwVXtWK2rh6S"},"source":["## Example - Solve POMDPs using a solver\n","\n","- We will reuse models we defined before.\n","- Agent is initialized with a belief distributed uniformly over all the states."]},{"cell_type":"code","execution_count":43,"metadata":{"id":"aHEZaWMnJpAw"},"outputs":[],"source":["def solve(\n","    init_true_state: PomdpState,\n","    steps: int = 15,\n","    planning_time: float = 0.5,\n","    max_depth: int = 10\n",") -> None:\n","    states = get_all_states(init_true_state)\n","    uniform_belief = {s: 1.0 / len(states) for s in states}\n","    init_belief = pomdp_py.Histogram(uniform_belief)\n","\n","    # 1. Select Agent, Environment\n","\n","    # Initialize agent, environment\n","    agent = pomdp_py.Agent(\n","        init_belief,\n","        PolicyModel(),\n","        TransitionModel(),\n","        ObservationModel(),\n","        RewardModel()\n","    )\n","    env = pomdp_py.Environment(\n","        init_true_state, TransitionModel(), RewardModel()\n","    )\n","\n","    # Initialize planners from library\n","    planner = pomdp_py.POUCT(\n","        max_depth=max_depth,\n","        discount_factor=0.95,\n","        planning_time=planning_time,\n","        exploration_const=110,\n","        rollout_policy=agent.policy_model,\n","    )\n","\n","    for i in range(steps):\n","        # 2. Agent plans an action at.\n","        print(\"\\n####################################\")\n","        print(f\"STEP {i}\")\n","        print(\"####################################\\n\")\n","        action = planner.plan(agent)\n","        print(f\"True state: \\n{env.state}\\n\")\n","        max_belief = agent.cur_belief.argmax()\n","        max_belief_prob = agent.cur_belief[max_belief]\n","        print(f\"Top Belief (Pr.= {round(max_belief_prob, 4)}) \\n{max_belief}\")\n","        print(f\"Action: {str(action)}\")\n","\n","        # 3. Environment state transitions\n","        # s_t -> s_t+1 according to its transition model.\n","        reward = env.state_transition(action, execute=True)\n","\n","        # 4. Agent receives an observation ot and reward\n","        # rt from the environment.\n","        observation = agent.observation_model.sample(env.state, action)\n","        print(f'Observation: \"{observation}\"')\n","        print(f\"Reward: {round(reward, 4)}\")\n","\n","        # 5.Agent updates history and belief\n","        agent.update_history(action, observation)\n","        planner.update(agent, action, observation)\n","        new_belief = pomdp_py.update_histogram_belief(\n","            agent.cur_belief,\n","            action,\n","            observation,\n","            agent.observation_model,\n","            agent.transition_model,\n","        )\n","        agent.set_belief(new_belief)"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"wo6x0Daxrh6S"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","####################################\n","STEP 0\n","####################################\n","\n","True state: \n","######\n","# #  #\n","# # G#\n","#^#  #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.0052) \n","######\n","# #  #\n","# # G#\n","#^#  #\n","#   G#\n","######\n","Action: l\n","Observation: \"# # \"\n","Reward: -0.1\n","\n","####################################\n","STEP 1\n","####################################\n","\n","True state: \n","######\n","# #  #\n","# # G#\n","#<#  #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.0417) \n","######\n","# #  #\n","# # G#\n","#<#  #\n","#   G#\n","######\n","Action: m\n","Observation: \"# # \"\n","Reward: -0.1\n","\n","####################################\n","STEP 2\n","####################################\n","\n","True state: \n","######\n","# #  #\n","# # G#\n","#<#  #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.0417) \n","######\n","# #  #\n","# # G#\n","#<#  #\n","#   G#\n","######\n","Action: l\n","Observation: \" # #\"\n","Reward: -0.1\n","\n","####################################\n","STEP 3\n","####################################\n","\n","True state: \n","######\n","# #  #\n","# # G#\n","#v#  #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.0417) \n","######\n","# #  #\n","# # G#\n","#^#  #\n","#   G#\n","######\n","Action: m\n","Observation: \"##  \"\n","Reward: -0.1\n","\n","####################################\n","STEP 4\n","####################################\n","\n","True state: \n","######\n","# #  #\n","# # G#\n","# #  #\n","#v  G#\n","######\n","\n","Top Belief (Pr.= 0.25) \n","######\n","# #  #\n","# # G#\n","# #  #\n","#v  G#\n","######\n","Action: l\n","Observation: \" ## \"\n","Reward: -0.1\n","\n","####################################\n","STEP 5\n","####################################\n","\n","True state: \n","######\n","# #  #\n","# # G#\n","# #  #\n","#>  G#\n","######\n","\n","Top Belief (Pr.= 0.25) \n","######\n","# #  #\n","# # G#\n","# #  #\n","#>  G#\n","######\n","Action: m\n","Observation: \" # #\"\n","Reward: -0.1\n","\n","####################################\n","STEP 6\n","####################################\n","\n","True state: \n","######\n","# #  #\n","# # G#\n","# #  #\n","# > G#\n","######\n","\n","Top Belief (Pr.= 0.25) \n","######\n","# #  #\n","# # G#\n","# #  #\n","# > G#\n","######\n","Action: m\n","Observation: \"G#  \"\n","Reward: -0.1\n","\n","####################################\n","STEP 7\n","####################################\n","\n","True state: \n","######\n","# #  #\n","# # G#\n","# #  #\n","#  >G#\n","######\n","\n","Top Belief (Pr.= 0.5) \n","######\n","# #  #\n","# # G#\n","# #  #\n","#  >G#\n","######\n","Action: m\n","Observation: \"##  \"\n","Reward: 1.0\n","\n","####################################\n","STEP 8\n","####################################\n","\n","True state: \n","######\n","# #  #\n","# # G#\n","# #  #\n","#   >#\n","######\n","\n","Top Belief (Pr.= 0.5) \n","######\n","# #  #\n","# # G#\n","# #  #\n","#   >#\n","######\n","Action: l\n","Observation: \" ## \"\n","Reward: -0.1\n","\n","####################################\n","STEP 9\n","####################################\n","\n","True state: \n","######\n","# #  #\n","# # G#\n","# #  #\n","#   ^#\n","######\n","\n","Top Belief (Pr.= 0.5) \n","######\n","# #  #\n","# # G#\n","# #  #\n","#   ^#\n","######\n","Action: m\n","Observation: \"G#  \"\n","Reward: -0.1\n","\n","####################################\n","STEP 10\n","####################################\n","\n","True state: \n","######\n","# #  #\n","# # G#\n","# # ^#\n","#    #\n","######\n","\n","Top Belief (Pr.= 1.0) \n","######\n","# #  #\n","# # G#\n","# # ^#\n","#    #\n","######\n","Action: m\n","Observation: \" #  \"\n","Reward: 1.0\n","\n","####################################\n","STEP 11\n","####################################\n","\n","True state: \n","######\n","# #  #\n","# # ^#\n","# #  #\n","#    #\n","######\n","\n","Top Belief (Pr.= 1.0) \n","######\n","# #  #\n","# # ^#\n","# #  #\n","#    #\n","######\n","Action: m\n","Observation: \"##  \"\n","Reward: -0.1\n","\n","####################################\n","STEP 12\n","####################################\n","\n","True state: \n","######\n","# # ^#\n","# #  #\n","# #  #\n","#    #\n","######\n","\n","Top Belief (Pr.= 1.0) \n","######\n","# # ^#\n","# #  #\n","# #  #\n","#    #\n","######\n","Action: m\n","Observation: \"##  \"\n","Reward: -0.1\n","\n","####################################\n","STEP 13\n","####################################\n","\n","True state: \n","######\n","# # ^#\n","# #  #\n","# #  #\n","#    #\n","######\n","\n","Top Belief (Pr.= 1.0) \n","######\n","# # ^#\n","# #  #\n","# #  #\n","#    #\n","######\n","Action: l\n","Observation: \" ## \"\n","Reward: -0.1\n","\n","####################################\n","STEP 14\n","####################################\n","\n","True state: \n","######\n","# # <#\n","# #  #\n","# #  #\n","#    #\n","######\n","\n","Top Belief (Pr.= 1.0) \n","######\n","# # <#\n","# #  #\n","# #  #\n","#    #\n","######\n","Action: m\n","Observation: \"##  \"\n","Reward: -0.1\n","\n","####################################\n","STEP 15\n","####################################\n","\n","True state: \n","######\n","# #< #\n","# #  #\n","# #  #\n","#    #\n","######\n","\n","Top Belief (Pr.= 1.0) \n","######\n","# #< #\n","# #  #\n","# #  #\n","#    #\n","######\n","Action: r\n","Observation: \"#  #\"\n","Reward: -0.1\n","\n","####################################\n","STEP 16\n","####################################\n","\n","True state: \n","######\n","# #^ #\n","# #  #\n","# #  #\n","#    #\n","######\n","\n","Top Belief (Pr.= 1.0) \n","######\n","# #^ #\n","# #  #\n","# #  #\n","#    #\n","######\n","Action: r\n","Observation: \"  ##\"\n","Reward: -0.1\n","\n","####################################\n","STEP 17\n","####################################\n","\n","True state: \n","######\n","# #> #\n","# #  #\n","# #  #\n","#    #\n","######\n","\n","Top Belief (Pr.= 1.0) \n","######\n","# #> #\n","# #  #\n","# #  #\n","#    #\n","######\n","Action: r\n","Observation: \" ## \"\n","Reward: -0.1\n","\n","####################################\n","STEP 18\n","####################################\n","\n","True state: \n","######\n","# #v #\n","# #  #\n","# #  #\n","#    #\n","######\n","\n","Top Belief (Pr.= 1.0) \n","######\n","# #v #\n","# #  #\n","# #  #\n","#    #\n","######\n","Action: m\n","Observation: \" #  \"\n","Reward: -0.1\n","\n","####################################\n","STEP 19\n","####################################\n","\n","True state: \n","######\n","# #  #\n","# #v #\n","# #  #\n","#    #\n","######\n","\n","Top Belief (Pr.= 1.0) \n","######\n","# #  #\n","# #v #\n","# #  #\n","#    #\n","######\n","Action: m\n","Observation: \" #  \"\n","Reward: -0.1\n"]}],"source":["maze = PomdpState([\n","    \"######\",\n","    \"# #  #\",\n","    \"# # G#\",\n","    \"#^#  #\",\n","    \"#   G#\",\n","    \"######\"])\n","\n","solve(maze, steps=20, planning_time=1.0, max_depth=20)"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"J-JKbtAyrh6S"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","####################################\n","STEP 0\n","####################################\n","\n","True state: \n","######\n","#    #\n","# ^  #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.0109) \n","######\n","#    #\n","# ^  #\n","#   G#\n","######\n","Action: l\n","Observation: \"    \"\n","Reward: -0.1\n","\n","####################################\n","STEP 1\n","####################################\n","\n","True state: \n","######\n","#    #\n","# <  #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.0625) \n","######\n","#    #\n","# ^  #\n","#   G#\n","######\n","Action: l\n","Observation: \"    \"\n","Reward: -0.1\n","\n","####################################\n","STEP 2\n","####################################\n","\n","True state: \n","######\n","#    #\n","# v  #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.0625) \n","######\n","#    #\n","# ^  #\n","#   G#\n","######\n","Action: l\n","Observation: \"    \"\n","Reward: -0.1\n","\n","####################################\n","STEP 3\n","####################################\n","\n","True state: \n","######\n","#    #\n","# >  #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.0625) \n","######\n","#    #\n","# ^  #\n","#   G#\n","######\n","Action: m\n","Observation: \"    \"\n","Reward: -0.1\n","\n","####################################\n","STEP 4\n","####################################\n","\n","True state: \n","######\n","#    #\n","#  > #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.25) \n","######\n","#    #\n","# <  #\n","#   G#\n","######\n","Action: r\n","Observation: \"    \"\n","Reward: -0.1\n","\n","####################################\n","STEP 5\n","####################################\n","\n","True state: \n","######\n","#    #\n","#  v #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.25) \n","######\n","#    #\n","# ^  #\n","#   G#\n","######\n","Action: r\n","Observation: \"    \"\n","Reward: -0.1\n","\n","####################################\n","STEP 6\n","####################################\n","\n","True state: \n","######\n","#    #\n","#  < #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.25) \n","######\n","#    #\n","# >  #\n","#   G#\n","######\n","Action: l\n","Observation: \"    \"\n","Reward: -0.1\n","\n","####################################\n","STEP 7\n","####################################\n","\n","True state: \n","######\n","#    #\n","#  v #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.25) \n","######\n","#    #\n","# ^  #\n","#   G#\n","######\n","Action: m\n","Observation: \"#  G\"\n","Reward: -0.1\n","\n","####################################\n","STEP 8\n","####################################\n","\n","True state: \n","######\n","#    #\n","#    #\n","#  vG#\n","######\n","\n","Top Belief (Pr.= 1.0) \n","######\n","#    #\n","#    #\n","#  vG#\n","######\n","Action: l\n","Observation: \"G#  \"\n","Reward: -0.1\n","\n","####################################\n","STEP 9\n","####################################\n","\n","True state: \n","######\n","#    #\n","#    #\n","#  >G#\n","######\n","\n","Top Belief (Pr.= 1.0) \n","######\n","#    #\n","#    #\n","#  >G#\n","######\n","Action: m\n","Observation: \"##  \"\n","Reward: 1.0\n","\n","####################################\n","STEP 10\n","####################################\n","\n","True state: \n","######\n","#    #\n","#    #\n","#   >#\n","######\n","\n","Top Belief (Pr.= 1.0) \n","######\n","#    #\n","#    #\n","#   >#\n","######\n","Action: m\n","Observation: \"##  \"\n","Reward: -0.1\n","\n","####################################\n","STEP 11\n","####################################\n","\n","True state: \n","######\n","#    #\n","#    #\n","#   >#\n","######\n","\n","Top Belief (Pr.= 1.0) \n","######\n","#    #\n","#    #\n","#   >#\n","######\n","Action: m\n","Observation: \"##  \"\n","Reward: -0.1\n","\n","####################################\n","STEP 12\n","####################################\n","\n","True state: \n","######\n","#    #\n","#    #\n","#   >#\n","######\n","\n","Top Belief (Pr.= 1.0) \n","######\n","#    #\n","#    #\n","#   >#\n","######\n","Action: m\n","Observation: \"##  \"\n","Reward: -0.1\n","\n","####################################\n","STEP 13\n","####################################\n","\n","True state: \n","######\n","#    #\n","#    #\n","#   >#\n","######\n","\n","Top Belief (Pr.= 1.0) \n","######\n","#    #\n","#    #\n","#   >#\n","######\n","Action: m\n","Observation: \"##  \"\n","Reward: -0.1\n","\n","####################################\n","STEP 14\n","####################################\n","\n","True state: \n","######\n","#    #\n","#    #\n","#   >#\n","######\n","\n","Top Belief (Pr.= 1.0) \n","######\n","#    #\n","#    #\n","#   >#\n","######\n","Action: l\n","Observation: \" ## \"\n","Reward: -0.1\n"]}],"source":["# Start in the middle.\n","# It takes some time for agent get reliable belief.\n","\n","maze = PomdpState([\n","    \"######\",\n","    \"#    #\",\n","    \"# ^  #\",\n","    \"#   G#\",\n","    \"######\"])\n","solve(maze, steps=15, planning_time=0.5, max_depth=20)"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"cQXlbVoqrh6T"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","####################################\n","STEP 0\n","####################################\n","\n","True state: \n","######\n","#^   #\n","#    #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.0109) \n","######\n","#^   #\n","#    #\n","#   G#\n","######\n","Action: m\n","Observation: \"#  #\"\n","Reward: -0.1\n","\n","####################################\n","STEP 1\n","####################################\n","\n","True state: \n","######\n","#^   #\n","#    #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.2) \n","######\n","#    #\n","#    #\n","#   v#\n","######\n","Action: m\n","Observation: \"#  #\"\n","Reward: -0.1\n","\n","####################################\n","STEP 2\n","####################################\n","\n","True state: \n","######\n","#^   #\n","#    #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.2) \n","######\n","#    #\n","#    #\n","#   v#\n","######\n","Action: m\n","Observation: \"#  #\"\n","Reward: -0.1\n","\n","####################################\n","STEP 3\n","####################################\n","\n","True state: \n","######\n","#^   #\n","#    #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.2) \n","######\n","#    #\n","#    #\n","#   v#\n","######\n","Action: r\n","Observation: \"  ##\"\n","Reward: -0.1\n","\n","####################################\n","STEP 4\n","####################################\n","\n","True state: \n","######\n","#>   #\n","#    #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.2) \n","######\n","#    #\n","#    #\n","#   <#\n","######\n","Action: m\n","Observation: \"   #\"\n","Reward: -0.1\n","\n","####################################\n","STEP 5\n","####################################\n","\n","True state: \n","######\n","# >  #\n","#    #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.2308) \n","######\n","#    #\n","#    #\n","#  < #\n","######\n","Action: r\n","Observation: \"  # \"\n","Reward: -0.1\n","\n","####################################\n","STEP 6\n","####################################\n","\n","True state: \n","######\n","# v  #\n","#    #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.2308) \n","######\n","#    #\n","#    #\n","#  ^ #\n","######\n","Action: m\n","Observation: \"    \"\n","Reward: -0.1\n","\n","####################################\n","STEP 7\n","####################################\n","\n","True state: \n","######\n","#    #\n","# v  #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.2308) \n","######\n","#    #\n","#  ^ #\n","#    #\n","######\n","Action: m\n","Observation: \"#   \"\n","Reward: -0.1\n","\n","####################################\n","STEP 8\n","####################################\n","\n","True state: \n","######\n","#    #\n","#    #\n","# v G#\n","######\n","\n","Top Belief (Pr.= 0.4286) \n","######\n","#  ^ #\n","#    #\n","#    #\n","######\n","Action: m\n","Observation: \"#   \"\n","Reward: -0.1\n","\n","####################################\n","STEP 9\n","####################################\n","\n","True state: \n","######\n","#    #\n","#    #\n","# v G#\n","######\n","\n","Top Belief (Pr.= 0.4286) \n","######\n","#  ^ #\n","#    #\n","#    #\n","######\n","Action: l\n","Observation: \" #  \"\n","Reward: -0.1\n","\n","####################################\n","STEP 10\n","####################################\n","\n","True state: \n","######\n","#    #\n","#    #\n","# > G#\n","######\n","\n","Top Belief (Pr.= 0.4286) \n","######\n","#  < #\n","#    #\n","#    #\n","######\n","Action: m\n","Observation: \"G#  \"\n","Reward: -0.1\n","\n","####################################\n","STEP 11\n","####################################\n","\n","True state: \n","######\n","#    #\n","#    #\n","#  >G#\n","######\n","\n","Top Belief (Pr.= 1.0) \n","######\n","#    #\n","#    #\n","#  >G#\n","######\n","Action: m\n","Observation: \"##  \"\n","Reward: 1.0\n","\n","####################################\n","STEP 12\n","####################################\n","\n","True state: \n","######\n","#    #\n","#    #\n","#   >#\n","######\n","\n","Top Belief (Pr.= 1.0) \n","######\n","#    #\n","#    #\n","#   >#\n","######\n","Action: m\n","Observation: \"##  \"\n","Reward: -0.1\n","\n","####################################\n","STEP 13\n","####################################\n","\n","True state: \n","######\n","#    #\n","#    #\n","#   >#\n","######\n","\n","Top Belief (Pr.= 1.0) \n","######\n","#    #\n","#    #\n","#   >#\n","######\n","Action: m\n","Observation: \"##  \"\n","Reward: -0.1\n","\n","####################################\n","STEP 14\n","####################################\n","\n","True state: \n","######\n","#    #\n","#    #\n","#   >#\n","######\n","\n","Top Belief (Pr.= 1.0) \n","######\n","#    #\n","#    #\n","#   >#\n","######\n","Action: l\n","Observation: \" ## \"\n","Reward: -0.1\n"]}],"source":["# Start near wall.\n","# Agent quickly gets reliable belief.\n","\n","maze = PomdpState([\n","    \"######\",\n","    \"#^   #\",\n","    \"#    #\",\n","    \"#   G#\",\n","    \"######\"])\n","solve(maze, steps=15, planning_time=0.5, max_depth=20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LhewHa9HQoLA"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1Dh73miPk2wgzHlwXQNgbA-S2MekLx1aN","timestamp":1709192916790}]},"kernelspec":{"display_name":"VENV_DIR","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"vscode":{"interpreter":{"hash":"28faef28e7f0745a706c61265858622dc1f2f4e731abb5496d55c478fc6bd5d9"}}},"nbformat":4,"nbformat_minor":0}
